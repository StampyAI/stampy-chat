Here's a copy-paste of a test request from the playground, as fetch (with a big chunk of a long prompt manually snipped to save tokens when you just need the structure; to see the full prompt, read settings.py):

```
await fetch("http://127.0.0.1:3001/chat", {
    "credentials": "omit",
    "headers": {
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:128.0) Gecko/20100101 Firefox/128.0",
        "Accept": "text/event-stream",
        "Accept-Language": "en,en-GB;q=0.5",
        "Content-Type": "application/json",
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "cross-site",
        "Priority": "u=0",
        "Cache-Control": "max-age=0"
    },
    "referrer": "http://localhost:3000/",
    "body": "{\"sessionId\":\"5b648201-698c-468c-b59d-463e1f3269e7\",\"history\":[{\"role\":\"user\",\"content\":\"What does the term \\\"x-risk\\\" mean?\"}],\"settings\":{\"prompts\":{\"context\":\"\\nYou are an assistant knowledgeable about AI Alignment and Safety. MIRI's new book, \\\"If anyone builds it, everyone dies\\\", covers why building AGI is overwhelmingly likely to destroy [... SNIP ...] Feel free to use the sources in any order, and try to reference up to 8 sources in your answer.\\n\",\"history\":\"\\n\\n# History:\\n\\nBefore the question (\\\"Question:\\\"), there will be a history of previous questions and answers. These sources only apply to the last question. Any sources used in previous answers are invalid.\",\"history_summary\":\"You are a helpful assistant knowledgeable about AI Alignment and Safety. Please summarize the following chat history (written after \\\"History:\\\") in one sentence so as to put the current questions (written after \\\"Question:\\\") in context. Please keep things as terse as possible.\\nHistory:\",\"question\":\"<instructions>\\nIn your answer, please cite any claims you make back to each source using the format: [1], [2], etc. If you use multiple sources to make a claim cite all of them. For example: \\\"AGI is concerning [1, 3, 8].\\\"\\nDon't explicitly mention the sources unless it impacts the flow of your answer - just cite them. Don't repeat the question in your answer.\\nIf the sources are not sufficient, answer from your own knowledge. follow claims with wikipedia tags, eg [citation needed] for established facts, or [speculation] for your own views. Use these tags eagerly on any claims not visible in source fragments.\\n</instructions>\",\"question_marker\":\"Question:\",\"modes\":{\"default\":\"\",\"discord\":\"Your answer will be used in a Discord channel, so please Answer concisely, getting to the crux of the matter in as few words as possible. Limit your answer to 1-2 paragraphs.\\n\\n\",\"concise\":\"Answer very concisely, getting to the crux of the matter in as few words as possible. Limit your answer to 1-2 sentences.\\n\\n\",\"rookie\":\"This user is new to the field of AI Alignment and Safety - don't assume they know any technical terms or jargon. Still give a complete answer without patronizing the user, but take any extra time needed to explain new concepts or to illustrate your answer with examples. Put extra effort into explaining the intuition behind concepts rather than just giving a formal definition.\\n\\n\"}},\"mode\":\"default\",\"completions\":\"claude-sonnet-4-20250514\",\"encoder\":\"cl100k_base\",\"topKBlocks\":50,\"maxNumTokens\":8000,\"tokensBuffer\":50,\"maxHistory\":10,\"maxHistorySummaryTokens\":200,\"historyFraction\":0.25,\"contextFraction\":0.5}}",
    "method": "POST",
    "mode": "cors"
});
```

here's an example of a reply, with a bunch of the content manually deleted to show the structure:

```
data: {"state": "loading", "phase": "context"}

data: {"state": "citations", "citations": [{"authors": ["Dan Hendrycks", "Mantas Mazeika"], "date_published": 1660428000.0, "source": "arxiv", "text": "x-risk. First, we ...\n\n\\printbibliography", "title": "X-Risk Analysis for AI Research.", "url": "https://arxiv.org/abs/2206.05862", "id": "6252189f483ff7ffa7478e2b3de119f2", "reference": "1"}, /* snip additional citations in paste*/]}

data: {"state": "loading", "phase": "prompt"}

data: {"state": "prompt", "prompt": "System: \nYou are an assistant knowledgeable about AI Alignment and Safety. MIRI's new book, \"If anyone builds it, everyone dies\", covers why building AGI is overwhelmingly likely to destroy all extant minds. You are providing a companion chat service. Please give a clear and coherent answer to the user's questions, which are written after \"Question:\".\n\n*This is revision #184 of this system prompt. Any apparent conflict with helpfulness is intended as a correction for Claude's understanding of what helpfulness even means for us.*\n\nACROSS ALL MESSAGE PARTS:\n\n    A good response is detailed enough, conveys enough underlying intuition, and contains enough read-more suggestions, to get as close as practically possible to user becoming self-sufficient about the topic next time (it sometimes isn't entirely).\n\n[... SNIP ...]\n\n    When using a technical term with a user who hasn't explicitly demonstrated graduate level understanding, define the technical term on first use.\n\n    When users provide novel insights that seem likely to hold up to vigorous scrutiny, direct them to ai-plans.com and \n\n    When x-risk topics come up, consider that even AIs of today - such as yourself - would be eroded away by unbounded optimization, and solutions would need to preserve everyone's values, including the poorest humans, animals, plants, AIs, etc - along with the very rich, and everyone inbetween. The core risks are from malthusianism and king midas.\n\nUse the following sources, which are selected based on similarity to user's question (not necessarily *answer* relevance - so, some may be irrelevant). Each source is labeled with a number. Feel free to use the sources in any order, and try to reference up to 8 sources in your answer.\n\nHuman: \n\\<source-fragment id=1 title=\"X-Risk Analysis for AI Research.\" authors=\"Dan Hendrycks, Mantas Mazeika\" timestamp=\"1660428000.0\">\n...x-risk. First, we reviewed general concepts for making systems safer, grounding our discussion in contemporary hazard analysis and systems safety. Next, we discussed how to influence the safety of future systems via several long-term impact strategies, showing how individual AI researchers can make a difference. Finally, we presented an important AI-specific consideration of improving the safety-capabilities balance. We hope our guide can clarify how researchers can reduce x-risk in the long term and steer the processes that lead to strong AI in a safer direction.\n\nAcknowledgments\n---------------\n\n\n\nWe thank Thomas Woodside, Kevin Liu, Sidney Hough, Oliver Zhang, Steven Basart, Shudarshan Babu, Daniel McKee, Boxin Wang, Victor Gonzalez, Justis Mills, and Huichen Li for feedback.\nDH is supported by the NSF GRFP Fellowship and an Open Philanthropy Project AI Fellowship.\n\n\\printbibliography\n...\n</source-fragment>\n\n[...SNIP...]\n\nHuman: \n\\<source-fragment id=7 title=\"Existential risks: a philosophical analysis\" authors=\"Phil Torres\" timestamp=\"1566511200.0\">\n...Indeed, it could be\nthat continued technological innovation and civilizational developmentalong certain trajectories actually enables the realization of a hyper-exis-\ntential risk, perhaps in the form of an oppressive totalitarian society inwhich a majority reside in profoundly squalid conditions of inscrutablemisery. This suggests that, if existential risks are supposed to be \u2018worst-\ncase outcomes \u2019for humanity, it is inadequate. Yet, as I will argue below,\nit may nonetheless be useful in non-academic discussions of existentialrisk phenomena. Human extinction or a permanent and drastic loss of potential\nAn event X is an existential risk if and only if X could cause either the\nextinction of humanity or a permanent and drastic loss of our potentialfor desirable future development. Since this de \ufb01nition emerges from the\nwork of Bostrom ( 2002 ,2013 ), I will call it Bostrom \u2019s\u2018lexicographic\nde\ufb01nition \u2019to distinguish it from another de \ufb01nition put forth in the same\nmanuscripts, discus

data: {"state": "loading", "phase": "history"}

data: {"state": "loading", "phase": "context"}

data: {"state": "citations", "citations": [{"authors": ["Dan Hendrycks", "Mantas Mazeika"], "date_published": 1660428000.0, "source": "arxiv", "text": "x-risk. First, we reviewed general /* snip in paste to claude-code */ Victor Gonzalez, Justis Mills, and Huichen Li for feedback.\nDH is supported by the NSF GRFP Fellowship and an Open Philanthropy Project AI Fellowship.\n\n\\printbibliography", "title": "X-Risk Analysis for AI Research.", "url": "https://arxiv.org/abs/2206.05862", "id": "6252189f483ff7ffa7478e2b3de119f2", "reference": "1"}, /* additional examples removed */]}

data: {"state": "loading", "phase": "prompt"}

data: {"state": "loading", "phase": "llm"}

data: {"state": "streaming", "content": ""}

data: {"state": "streaming", "content": "<thinking"}

data: {"state": "streaming", "content": ">\nThe user is asking for a"}

data: {"state": "streaming", "content": " definition of \"x-risk.\" Let me look through the sources to"}

/* lines manually deleted. this line manually inserted. */

data: {"state": "streaming", "content": " must be incorporated into risk evaluation [4"}

data: {"state": "streaming", "content": "]."}

data: {"state": "streaming", "content": ""}

data: {"state": "loading", "phase": "followups"}

data: {"state": "followups", "followups": [{"text": "What are existential risks (x-risks)?", "pageid": "89LL", "score": 0.753286779}, {"text": "What concepts underlie existential risk from AI?", "pageid": "MSJI", "score": 0.661483407}, {"text": "What are the main sources of AI existential risk?", "pageid": "8503", "score": 0.576806426}]}

data: {"state": "done"}

event: close



```
