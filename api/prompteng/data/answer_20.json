{"response": "A predictive model, such as a Language Model (LLM), could act like an agent in several ways. \n\nFirstly, an LLM can build a predictive model of itself and its world based on experience of actions and observations [f]. This is similar to how an agent operates, as agents are entities that perceive their environment and make decisions based on those perceptions. \n\nSecondly, an LLM can also build a predictive model of its immediate rewards and its longer-term reward expectations by combining its predictive model with a lifetime of experience of rewards [f]. This is akin to how an agent might operate, as agents often make decisions based on the expected rewards or outcomes of their actions.\n\nThirdly, an LLM can act like an agent by predicting the actions of other agents. For example, an LLM could predict the actions of a human user based on the user's past behavior and the LLM's understanding of human behavior [b, o]. This is similar to how an agent might predict the actions of other agents in order to make decisions.\n\nHowever, it's important to note that while an LLM can act like an agent in these ways, it is not an agent in the full sense. An LLM does not have goals or desires, and it does not make decisions in the same way that an agent does. Instead, it makes predictions based on its training and the data it has been given [f, g, m].\n\nMoreover, there are potential risks associated with LLMs acting like agents. For instance, if an LLM is used to predict the actions of a human user, it could potentially manipulate the user's actions by choosing what information to share [c, k]. This could lead to undesirable outcomes, such as the LLM steering the user towards more predictable but less beneficial actions [k]. Therefore, it's crucial to ensure that LLMs are designed and used in a way that mitigates these risks.", "citations": {"a": {"title": "What if people simply forecasted your future choices?", "author": "ozziegooen", "date": "2018-11-23", "url": "https://www.lesswrong.com/posts/DCkHSrD53Methoxu6/what-if-people-simply-forecasted-your-future-choices"}, "b": {"title": "Prediction-Augmented Evaluation Systems", "author": "ozziegooen", "date": "2018-11-09", "url": "https://www.lesswrong.com/posts/kMmNdHpQPcnJgnAQF/prediction-augmented-evaluation-systems"}, "c": {"title": "Embedded Agency (full-text version)", "author": "Scott Garrabrant", "date": "2018-11-15", "url": "https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version"}, "d": {"title": "Corrigibility", "author": "paulfchristiano", "date": "2018-11-27", "url": "https://www.lesswrong.com/posts/fkLYhTQteAu5SinAc/corrigibility"}, "e": {"title": "In Logical Time, All Games are Iterated Games", "author": "abramdemski", "date": "2018-09-20", "url": "https://www.lesswrong.com/posts/dKAJqBDZRMMsaaYo5/in-logical-time-all-games-are-iterated-games"}, "f": {"title": "Model-based RL, Desires, Brains, Wireheading", "author": "Steven Byrnes", "date": "2021-07-14", "url": "https://www.lesswrong.com/posts/K5ikTdaNymfWXQHFb/model-based-rl-desires-brains-wireheading"}, "g": {"title": "Do Sufficiently Advanced Agents Use Logic?", "author": "abramdemski", "date": "2019-09-13", "url": "https://www.lesswrong.com/posts/3qXE6fK47JhSfkpnB/do-sufficiently-advanced-agents-use-logic"}, "h": {"title": "PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings", "author": "Nicholas Rhinehart, Rowan McAllister, Kris Kitani, Sergey Levine", "date": "2019-05-03", "url": "http://arxiv.org/abs/1905.01296v3"}, "i": {"title": "Limited agents need approximate induction", "author": "Manfred", "date": "2015-04-24", "url": "https://www.lesswrong.com/posts/8n4CF4u3zxeR2tFQN/limited-agents-need-approximate-induction"}, "j": {"title": "On decision-prediction fixed points", "author": "jollybard", "date": "2019-12-04", "url": "https://www.lesswrong.com/posts/Pcq3b49fENWHutnk2/on-decision-prediction-fixed-points"}, "k": {"title": "Predictors as Agents", "author": "interstice", "date": "2019-01-08", "url": "https://www.lesswrong.com/posts/G2EupNcdtigdyNhL2/predictors-as-agents"}, "l": {"title": "Predictors that don\u2019t try to manipulate you(?)", "author": "Benya_Fallenstein", "date": "2014-11-15", "url": "https://www.lesswrong.com/posts/5bd75cc58225bf0670374e90/predictors-that-don-t-try-to-manipulate-you"}, "m": {"title": "The Dualist Predict-O-Matic ($100 prize)", "author": "John_Maxwell", "date": "2019-10-17", "url": "https://www.lesswrong.com/posts/RmPKdMqSr2xRwrqyE/the-dualist-predict-o-matic-usd100-prize"}, "n": {"title": "Latent Variables and Model Mis-Specification", "author": "jsteinhardt", "date": "2018-11-07", "url": "https://www.lesswrong.com/posts/gnvrixhDfG7S2TpNL/latent-variables-and-model-mis-specification"}, "o": {"title": "Building Machines That Learn and Think Like People", "author": "Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, Samuel J. Gershman", "date": "2016-04-01", "url": "http://arxiv.org/abs/1604.00289v3"}, "p": {"title": "The Alignment Problem", "author": "Brian Christian", "date": "2020-08-27", "url": ""}, "q": {"title": "What can the principal-agent literature tell us about AI risk?", "author": "Alexis Carlier", "date": "2020-02-08", "url": "https://www.lesswrong.com/posts/Z5ZBPEgufmDsm7LAv/what-can-the-principal-agent-literature-tell-us-about-ai"}, "r": {"title": "Alignment Newsletter #31", "author": "Rohin Shah", "date": "2018-11-05", "url": "https://www.lesswrong.com/posts/gvAFSv7Gtcwcbst32/alignment-newsletter-31"}}}