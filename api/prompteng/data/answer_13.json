{"response": "The term \"t-AGI\" or \"tool-AGI\" refers to a conceptualization of artificial general intelligence (AGI) where the AGI operates as a tool rather than an autonomous agent [g]. In this framework, the AGI is designed to provide transparent views into its reasoning and predictions, without any risk of being purposefully misled. It also facilitates safe experimental testing of any utility function that one wishes to eventually plug into an \"agent\" [g].\n\nThe t-AGI framework is based on the idea that intelligence inherently involves two distinct, separable steps: (a) considering multiple possible actions and (b) assigning a score to each, prior to executing any of the possible actions. If one can distinctly separate (a) and (b) in a program\u2019s code, then one can abstain from writing any \"execution\" instructions and instead focus on making the program list actions and scores in a user-friendly manner, for humans to consider and use as they wish [g].\n\nThis approach to AGI is distinct from the concept of agentic AGI, where the AGI is designed to act autonomously towards achieving its goals. The t-AGI framework is seen as a safer approach to AGI development, as it allows for greater transparency and control over the AGI's actions [g].", "citations": {"a": {"title": "A Metamodel and Framework for Artificial General Intelligence From Theory to Practice", "author": "Hugo Latapie, Ozkan Kilic, Gaowen Liu, Yan Yan, Ramana Kompella, Pei Wang, Kristinn R. Thorisson, Ad", "date": "2021-02-11", "url": "http://arxiv.org/abs/2102.06112v1"}, "b": {"title": "A Metamodel and Framework for AGI", "author": "Hugo Latapie, Ozkan Kilic", "date": "2020-08-28", "url": "http://arxiv.org/abs/2008.12879v2"}, "c": {"title": "My current framework for thinking about AGI timelines", "author": "zhukeepa", "date": "2020-03-30", "url": "https://www.lesswrong.com/posts/w4jjwDPa853m9P4ag/my-current-framework-for-thinking-about-agi-timelines"}, "d": {"title": "AGI safety from first principles: Alignment", "author": "Richard_Ngo", "date": "2020-10-01", "url": "https://www.lesswrong.com/posts/PvA2gFMAaHCHfMXrw/agi-safety-from-first-principles-alignment"}, "e": {"title": "MIRI\u2019s 2017 Fundraiser", "author": "Malo", "date": "2017-12-07", "url": "https://www.lesswrong.com/posts/shy6XPhbrh9az6NG6/miri-s-2017-fundraiser"}, "f": {"title": "MIRI\u2019s 2017 Fundraiser", "author": "Malo", "date": "2017-12-01", "url": "https://www.lesswrong.com/posts/D7SFzsTNTw7KJy9cn/miri-s-2017-fundraiser"}, "g": {"title": "Holden Karnofsky\u2019s Singularity Institute Objection 2", "author": "Paul Crowley", "date": "2012-05-11", "url": "https://www.lesswrong.com/posts/TJoPCusrS4cNMgy3n/holden-karnofsky-s-singularity-institute-objection-2"}, "h": {"title": "Question 1: Predicted architecture of AGI learning algorithm(s)", "author": "Cameron Berg", "date": "2022-02-10", "url": "https://www.lesswrong.com/posts/snwpyAfzoFKdfnEDj/question-1-predicted-architecture-of-agi-learning-algorithm"}, "i": {"title": "The Problem With The Current State of AGI Definitions", "author": "Yitz", "date": "2022-05-29", "url": "https://www.lesswrong.com/posts/EpR5yTZMaJkDz4hhs/the-problem-with-the-current-state-of-agi-definitions"}, "j": {"title": "AI Governance: A Research Agenda", "author": "Allan Dafoe", "date": "", "url": ""}, "k": {"title": "Paradigm-building: Conclusion and practical takeaways", "author": "Cameron Berg", "date": "2022-02-15", "url": "https://www.lesswrong.com/posts/HpHHJaHrdpimpBvv5/paradigm-building-conclusion-and-practical-takeaways"}, "l": {"title": "Paradigm-building: The hierarchical question framework", "author": "Cameron Berg", "date": "2022-02-09", "url": "https://www.lesswrong.com/posts/smLEgf2tHsS5NHAf6/paradigm-building-the-hierarchical-question-framework"}, "m": {"title": "The AI Does Not Hate You: Superintelligence, Rationality and the Race to Save the World", "author": "Tom Chivers", "date": "2019-06-13", "url": ""}, "n": {"title": "AI Safety Reading Group (Session 50)-by AI Safety Reading Group-video_id Tnnn6Lt", "author": "S\u00f8ren Elverlin", "date": "", "url": ""}, "o": {"title": "Overview of Artificial General Intelligence Safety Research Agendas _ Rohin Shah", "author": "Question from Audience, Rohin Shah, interviewer", "date": "", "url": ""}, "p": {"title": "[Intro to brain-like-AGI safety] 3. Two subsystems: Learning & Steering", "author": "Steven Byrnes", "date": "2022-02-09", "url": "https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"}}}