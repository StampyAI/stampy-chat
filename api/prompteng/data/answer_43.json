{"response": "Narrow AI, also known as weak AI, is designed to perform a specific task, such as playing chess or driving a car [a, e]. It operates within a limited domain and cannot reason outside the domains it was designed for [b]. Examples of narrow AI include IBM's Watson, Google's AlphaGo, and the calculator in your phone [c, f]. Despite its limitations, narrow AI can be quite useful and has found applications across a wide range of contexts [b]. However, it does not have the capability to generalize its learning to other domains or tasks [e, l].\n\nOn the other hand, General AI, also known as Artificial General Intelligence (AGI) or strong AI, is intelligent across a wide range of domains [b, e]. It is designed to perform any intellectual task that a human being can do [e]. Unlike narrow AI, AGI can learn new things and apply its learning across different areas without requiring a fundamental rewriting of its code [c]. It is capable of generalizing its learning, transferring what it has learned in one subject to another, and drawing connections that enable it to become smarter [l].\n\nThe distinction between narrow and general AI is not binary but rather a spectrum, with degrees of generality [a, k]. For instance, large language models like GPT-3 have a fairly large degree of generality without being as general as a human [a, d]. Similarly, DeepMind's AlphaZero and MuZero are more general than a chess-specific AI like Stockfish, as they can be trained to play multiple games [f, k].\n\nIt's important to note that while we have made significant progress in narrow AI, the development of AGI remains a challenging and active area of research [b, j]. The ultimate goal of AI research is to create a system that needs no problem-specific engineering and can simply be asked to perform any task, like teaching a molecular biology class or running a government [g].", "citations": {"a": {"title": "Narrow AI", "author": "plex", "date": "", "url": "https://greaterwrong.com/tag/narrow-ai?showPostCount=true&useTagName=true"}, "b": {"title": "The great downside dilemma for risky emerging technologies", "author": "Seth Baum", "date": "", "url": "10.1088/0031-8949/89/12/128004"}, "c": {"title": "Sam Harris and Eliezer Yudkowsky on \u201cAI: Racing Toward the Brink\u201d", "author": "Rob Bensinger", "date": "2018-03-01", "url": "https://intelligence.org/2018/02/28/sam-harris-and-eliezer-yudkowsky/"}, "d": {"title": "Review of \"Why AI is Harder Than We Think\"", "author": "electroswing", "date": "2021-04-30", "url": "https://www.lesswrong.com/posts/FNqgoSg6cEKf5i6Bt/review-of-why-ai-is-harder-than-we-think"}, "e": {"title": "Nonparametric General Reinforcement Learning", "author": "Jan Leike", "date": "2016-11-28", "url": "http://arxiv.org/abs/1611.08944v1"}, "f": {"title": "The Case for Artificial Expert Intelligence (AXI): What lies between narrow and general AI?", "author": "Yuli_Ban", "date": "2020-02-02", "url": "https://www.lesswrong.com/posts/wGJo9xDicwwppxDJt/the-case-for-artificial-expert-intelligence-axi-what-lies"}, "g": {"title": "Human Compatible", "author": "Stuart Russell", "date": "2019-10-08", "url": ""}, "h": {"title": "Q&A with experts on risks from AI #3", "author": "XiXiDu", "date": "2012-01-12", "url": "https://www.lesswrong.com/posts/Jv9kyH5WvqiXifsWJ/q-and-a-with-experts-on-risks-from-ai-3"}, "i": {"title": "Subjectivity Learning Theory towards Artificial General Intelligence", "author": "Xin Su, Shangqi Guo, Feng Chen", "date": "2019-09-09", "url": "http://arxiv.org/abs/1909.03798v2"}, "j": {"title": "How long until human-level AI? Results from an expert assessment", "author": "Seth Baum, Ben Goertzel, Ted Goertzel", "date": "", "url": "10.1016/j.techfore.2010.09.006"}, "k": {"title": "AGI Safety and Alignment with Robert Miles on the Machine Ethics Podcast", "author": "Machine Ethics", "date": "2021-01-13", "url": ""}, "l": {"title": "End Times: A Brief Guide to the End of the World", "author": "Bryan Walsh", "date": "2019-08-27", "url": ""}, "m": {"title": "Some thoughts on risks from narrow, non-agentic AI", "author": "Richard_Ngo", "date": "2021-01-19", "url": "https://www.lesswrong.com/posts/AWbtbmC6rAg6dh75b/some-thoughts-on-risks-from-narrow-non-agentic-ai"}, "n": {"title": "AXRP Episode 13 - First Principles of AGI Safety with Richard Ngo", "author": "DanielFilan", "date": "2022-03-31", "url": "https://www.lesswrong.com/posts/tEf8fEFCkFtPyg9pm/axrp-episode-13-first-principles-of-agi-safety-with-richard"}, "o": {"title": "Narrow AI Nanny:  Reaching Strategic Advantage via Narrow AI to Prevent Creation of the Dangerous Su", "author": "avturchin", "date": "2018-07-25", "url": "https://www.lesswrong.com/posts/7ysKDyQDPK3dDAbkT/narrow-ai-nanny-reaching-strategic-advantage-via-narrow-ai"}, "p": {"title": "Deep Learning and Artificial General Intelligence: Still a Long Way to Go", "author": "Maciej \u015awiechowski", "date": "2022-03-25", "url": "http://arxiv.org/abs/2203.14963v2"}}}