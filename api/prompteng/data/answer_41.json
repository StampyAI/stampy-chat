{"response": "Prosaic AI alignment refers to the process of aligning artificial general intelligence (AGI) developed using current or \"prosaic\" AI methods with human values and goals. These methods are the ones we are using today, rather than hypothetical new methods based on a deeper understanding of intelligence [b]. The goal of prosaic AI alignment is to ensure that AGI, even if developed using these prosaic methods, behaves in a way that is beneficial to humanity and does not pose existential risks [a].\n\nThe prosaic AI alignment project can also be seen as an attempt to prevent malign generalization error from ever happening, even as we try to eliminate benign errors. This is challenging because moving toward robust generalization and toward malign generalization seem very similar, and you need some way to differentially advantage the first [e].\n\nProsaic AI alignment is considered important for several reasons. First, there is a reasonable chance that we will build AGI using prosaic methods. Second, even if we don't develop prosaic AGI, it is very likely that there will be important similarities between alignment for prosaic AGI and alignment for whatever kind of AGI we actually build [a, c]. \n\nHowever, it's important to note that aligning prosaic AGI is considered challenging, as it may require a deep understanding of intelligence to build aligned AI [c]. There are also concerns about the competitiveness of AGI developed using prosaic methods, as cutting-edge AI is expected to come from cutting-edge algorithms and architectures trained towards cutting-edge objectives in cutting-edge environments/datasets [b].", "citations": {"a": {"title": "Prosaic AI alignment", "author": "paulfchristiano", "date": "2018-11-20", "url": "https://www.lesswrong.com/posts/YTq4X6inEudiHkHDF/prosaic-ai-alignment"}, "b": {"title": "A dilemma for prosaic AI alignment", "author": "Daniel Kokotajlo", "date": "2019-12-17", "url": "https://www.lesswrong.com/posts/jYdAxH8BarPT4fqnb/a-dilemma-for-prosaic-ai-alignment"}, "c": {"title": "Alignment Newsletter #34", "author": "Rohin Shah", "date": "2018-11-26", "url": "https://www.lesswrong.com/posts/ZFT78ezD2yxLjo6QM/alignment-newsletter-34"}, "d": {"title": "Alignment versus AI Alignment", "author": "Alex Flint", "date": "2022-02-04", "url": "https://www.lesswrong.com/posts/hTfyX4823wKqnoFnS/alignment-versus-ai-alignment"}, "e": {"title": "Tabooing \u2018Agent\u2019 for Prosaic Alignment", "author": "Hjalmar_Wijk", "date": "2019-08-23", "url": "https://www.lesswrong.com/posts/zCcmJzbenAXu6qugS/tabooing-agent-for-prosaic-alignment"}, "f": {"title": "The case for aligning narrowly superhuman models", "author": "Ajeya Cotra", "date": "2021-03-05", "url": "https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models"}, "g": {"title": "A positive case for how we might succeed at prosaic AI alignment", "author": "evhub", "date": "2021-11-16", "url": "https://www.lesswrong.com/posts/5ciYedyQDDqAcrDLr/a-positive-case-for-how-we-might-succeed-at-prosaic-ai"}, "h": {"title": "Challenges to Christiano\u2019s capability amplification proposal", "author": "Eliezer Yudkowsky", "date": "2018-05-19", "url": "https://intelligence.org/2018/05/19/challenges-to-christianos-capability-amplification-proposal/"}, "i": {"title": "Challenges to Christiano\u2019s capability amplification proposal", "author": "Eliezer Yudkowsky", "date": "2018-05-19", "url": "https://www.lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal"}}}