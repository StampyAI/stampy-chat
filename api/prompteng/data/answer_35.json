{"response": "AI alignment, also referred to as the alignment problem for advanced agents, Friendly AI, or robust and beneficial AI, is the research field focused on developing sufficiently advanced machine intelligences that produce good outcomes in the real world [b]. It concerns the task of ensuring that artificially intelligent systems behave in a way that is conducive to human morality and human wellbeing [a]. \n\nThe problem of AI alignment arises from the orthogonality thesis, which suggests that intelligence and goals can exist in any combination. This means that an AI's intelligence level does not necessarily dictate its goals or behavior [a]. \n\nAI alignment involves aligning the goals of artificial systems with the broader goals of humanity [k]. This includes both intent alignment, where the AI tries to do what a human or institution wants it to do, and impact alignment, where the AI succeeds in doing what is wanted [f]. \n\nThe alignment problem also includes the challenge of designing the reward or utility function or values that the AI seeks to optimize [c]. Various approaches have been proposed to address this problem, including Cooperative Inverse Reinforcement Learning (CIRL), where the AI system learns its reward function through interaction with a human [d], and Iterated Distillation and Amplification (IDA), which involves iteratively training AI systems to learn from human experts assisted by AI helpers [d].\n\nAI alignment is a complex and ongoing area of research, requiring deep thought and hard work to figure out how best to constrain artificial intelligence [a]. It is considered crucial for ensuring a beneficial coexistence with AI systems whose intelligence is growing at an exponential rate [c].", "citations": {"a": {"title": "What is AI alignment-by Future Explained-video_id NnDermRo8O0-date 20180703", "author": "Future Explained", "date": "", "url": ""}, "b": {"title": "AI alignment", "author": "arbital", "date": "2015-03-26", "url": "https://arbital.com/"}, "c": {"title": "AI Alignment Overview - Ensuring Artificial Intelligence Behaves-by Matthew Voke", "author": "Matthew Voke, Stuart Russell", "date": "", "url": ""}, "d": {"title": "Garrabrant and Shah on human modeling in AGI", "author": "Rob Bensinger", "date": "2021-08-04", "url": "https://www.lesswrong.com/posts/Wap8sSDoiigrJibHA/garrabrant-and-shah-on-human-modeling-in-agi"}, "e": {"title": "[AN #95]: A framework for thinking about how to make AI go well", "author": "Rohin Shah", "date": "2020-04-15", "url": "https://www.lesswrong.com/posts/9et86yPRk6RinJNt3/an-95-a-framework-for-thinking-about-how-to-make-ai-go-well"}, "f": {"title": "Some AI research areas and their relevance to existential safety", "author": "Andrew_Critch", "date": "2020-11-19", "url": "https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1"}, "g": {"title": "Towards an Axiological Approach to AI Alignment", "author": "G Gordon Worley III", "date": "2017-11-15", "url": "https://www.lesswrong.com/posts/NwxjvegAbLaBJ3TvC/towards-an-axiological-approach-to-ai-alignment-1"}, "h": {"title": "[AN #85]: The normative questions we should be asking for AI alignment, and a surprisingly good chat", "author": "Rohin Shah", "date": "2020-02-05", "url": "https://www.lesswrong.com/posts/Mj259G5n5BxXXrZ7C/an-85-the-normative-questions-we-should-be-asking-for-ai"}, "i": {"title": "On the Ethics of Building AI in a Responsible Manner", "author": "Shai Shalev-Shwartz, Shaked Shammah, Amnon Shashua", "date": "2020-03-30", "url": "http://arxiv.org/abs/2004.04644v1"}, "j": {"title": "Alignment versus AI Alignment", "author": "Alex Flint", "date": "2022-02-04", "url": "https://www.lesswrong.com/posts/hTfyX4823wKqnoFnS/alignment-versus-ai-alignment"}, "k": {"title": "How harmful are improvements in AI? + Poll", "author": "tilker", "date": "2022-02-15", "url": "https://www.lesswrong.com/posts/uABbabv5WPZmwzCmP/how-harmful-are-improvements-in-ai-poll"}, "l": {"title": "[AN #74]: Separating beneficial AI into competence, alignment, and coping with impacts", "author": "Rohin Shah", "date": "2019-11-20", "url": "https://www.lesswrong.com/posts/X2fRsTjd2kQ89pipE/an-74-separating-beneficial-ai-into-competence-alignment-and"}, "m": {"title": "A General Language Assistant as a Laboratory for Alignment", "author": "Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas J", "date": "2021-12-01", "url": "http://arxiv.org/abs/2112.00861v3"}, "n": {"title": "The Alignment Newsletter #1: 04/\u200b09/\u200b18", "author": "Rohin Shah", "date": "2018-04-09", "url": "https://www.lesswrong.com/posts/CsMQ7zsprBqWaeSvk/the-alignment-newsletter-1-04-09-18"}, "o": {"title": "Introducing the AI Alignment Forum (FAQ)", "author": "habryka", "date": "2018-10-29", "url": "https://www.lesswrong.com/posts/FoiiRDC3EhjHx7ayY/introducing-the-ai-alignment-forum-faq"}, "p": {"title": "AI Alignment in AI Trustworthy-by Fanchen Bao-video_id UyOk2SxkKYc-date 20210329", "author": "Fanchen Bao", "date": "", "url": ""}, "q": {"title": "Welcome & FAQ!", "author": "Ruby", "date": "2021-08-24", "url": "https://www.lesswrong.com/posts/Yp2vYb4zHXEeoTkJc/welcome-and-faq"}}}