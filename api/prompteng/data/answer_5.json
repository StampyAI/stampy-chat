{"response": "The Orthogonality Thesis is a concept in artificial intelligence (AI) that asserts that an AI's intelligence (its ability to achieve goals) and its final goals can vary independently of each other [a, e]. In other words, it suggests that an AI can have any combination of intelligence level and goal, and that its level of intelligence does not necessarily determine its goals [e]. This is in contrast to the belief that, due to their intelligence, all AIs will converge to a common goal [e]. \n\nThe Orthogonality Thesis is a descriptive statement about the logical design space of possible cognitive agents, not a normative assertion about how things ought to be [a]. It does not require that all agent designs be equally compatible with all goals, but rather states that there exists at least one possible agent that can pursue any given goal [a]. \n\nThe thesis is often used to argue against the instinctive belief that if an AI were smart enough, it would figure out the \"right\" morality and everything would be fine [b]. It is a key premise in two important policy-relevant propositions: that it is possible to build a \"nice\" AI, and that it is possible to make a mistake when trying to build a \"nice\" AI, in which case the AI will not automatically decide to be nice instead [a]. \n\nThe Orthogonality Thesis was originally defined by Nick Bostrom [e] and has been defended by Stuart Armstrong [f, h]. It is considered by some to be the default position in discussions about AI, with the burden of proof being on claims that limit possible AIs [e, k].", "citations": {"a": {"title": "Orthogonality Thesis", "author": "arbital", "date": "2015-03-10", "url": "https://arbital.com/"}, "b": {"title": "General purpose intelligence: arguing the Orthogonality thesis", "author": "Stuart_Armstrong", "date": "2012-05-15", "url": "https://www.lesswrong.com/posts/nvKZchuTW8zY6wvAj/general-purpose-intelligence-arguing-the-orthogonality"}, "c": {"title": "How many philosophers accept the orthogonality thesis ? Evidence from the PhilPapers survey", "author": "Paperclip Minimizer", "date": "2018-06-16", "url": "https://www.lesswrong.com/posts/LdENjKB6G6fZDRZFw/how-many-philosophers-accept-the-orthogonality-thesis"}, "d": {"title": "Are we all misaligned?", "author": "Mateusz Mazurkiewicz", "date": "2021-01-03", "url": "https://www.lesswrong.com/posts/Lshuoww97Loy2h7kw/are-we-all-misaligned-1"}, "e": {"title": "Orthogonality Thesis", "author": "Yoav Ravid", "date": "", "url": "https://greaterwrong.com/tag/orthogonality-thesis?showPostCount=true&useTagName=true"}, "f": {"title": "Arguing Orthogonality, published form", "author": "Stuart_Armstrong", "date": "2013-03-18", "url": "https://www.lesswrong.com/posts/AJ3aP8iWxr6NaKi6j/arguing-orthogonality-published-form"}, "g": {"title": "John Danaher on \u2018The Superintelligent Will\u2019", "author": "lukeprog", "date": "2012-04-03", "url": "https://www.lesswrong.com/posts/DDxntjTYjRbZF9xTd/john-danaher-on-the-superintelligent-will"}, "h": {"title": "GENERAL PURPOSE INTELLIGENCE: ARGUING THE ORTHOGONALITY THESIS", "author": "Stuart Armstrong", "date": "", "url": ""}, "i": {"title": "Evidence for the orthogonality thesis", "author": "Stuart_Armstrong", "date": "2012-04-03", "url": "https://www.lesswrong.com/posts/CRsYy3xtbMrLjoXZT/evidence-for-the-orthogonality-thesis"}, "j": {"title": "Is the Orthogonality Thesis Defensible if We Assume Both Valence Realism and Open Individualism?", "author": "algekalipso", "date": "2019-11-09", "url": "https://qualiacomputing.com/2019/11/09/is-the-orthogonality-thesis-defensible-if-we-assume-both-valence-realism-and-open-individualism/"}, "k": {"title": "The AI Does Not Hate You: Superintelligence, Rationality and the Race to Save the World", "author": "Tom Chivers", "date": "2019-06-13", "url": ""}, "l": {"title": "THE SUPERINTELLIGENT WILL: MOTIVATION AND INSTRUMENTAL RATIONALITY IN ADVANCED ARTIFICIAL AGENTS", "author": "Nick Bostrom", "date": "", "url": ""}}}