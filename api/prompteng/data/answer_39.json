{"response": "Inverse Reinforcement Learning (IRL) is a framework for learning a reward function from the actions of an expert, often a human demonstrator [j]. It refers to both the problem and method by which an AI agent learns preferences of another agent (often considered an \"expert\") that explain the latter's observed behavior [a]. The learning agent is assumed to perfectly know the parameters of the Markov Decision Process (MDP) except the reward function, and its task is to find a reward function under which the expert's observed behavior is optimal [a]. \n\nIRL is a paradigm relying on Markov Decision Processes, where an apprentice AI agent is given a set of demonstrations from an expert solving some problem and its goal is to find a reward function that best explains the expert's behavior [f]. \n\nHowever, it's important to note that this problem is generally ill-posed because for any given behavior there are infinitely-many reward functions which align with the behavior [a]. Therefore, additional structure or properties are often imposed on the learned rewards to address this issue [l]. \n\nIRL is often used in imitation learning, where the AI system learns from demonstrations by a human teacher in controlled environments [a, g]. It has been applied to a number of problems, most related to the problems of learning from demonstrations and apprenticeship learning [l].", "citations": {"a": {"title": "A Framework and Method for Online Inverse Reinforcement Learning", "author": "Saurabh Arora, Prashant Doshi, Bikramjit Banerjee", "date": "2018-05-21", "url": "http://arxiv.org/abs/1805.07871v1"}, "b": {"title": "Model Mis-specification and Inverse Reinforcement Learning", "author": "jsteinhardt", "date": "2017-02-07", "url": "https://jsteinhardt.wordpress.com/2017/02/07/model-mis-specification-and-inverse-reinforcement-learning/"}, "c": {"title": "The Alignment Problem", "author": "Brian Christian", "date": "2020-08-27", "url": ""}, "d": {"title": "Human Compatible", "author": "Stuart Russell", "date": "2019-10-08", "url": ""}, "e": {"title": "Model Mis-specification and Inverse Reinforcement Learning", "author": "Owain_Evans", "date": "2018-11-09", "url": "https://www.lesswrong.com/posts/cnC2RMWEGiGpJv8go/model-mis-specification-and-inverse-reinforcement-learning"}, "f": {"title": "The Concept of Criticality in AI Safety", "author": "Yitzhak Spielberg, Amos Azaria", "date": "2022-01-12", "url": "http://arxiv.org/abs/2201.04632v1"}, "g": {"title": "What is narrow value learning?", "author": "Rohin Shah", "date": "2019-01-10", "url": "https://www.lesswrong.com/posts/vX7KirQwHsBaSEdfK/what-is-narrow-value-learning"}, "h": {"title": "EAI 2022 AGI Safety Fundamentals alignment curriculum", "author": "EleutherAI, Richard Ngo", "date": "2022-05-16", "url": ""}, "i": {"title": "Goodhart Ethology", "author": "Charlie Steiner", "date": "2021-09-17", "url": "https://www.lesswrong.com/posts/z2BPxcFfhKho89D8L/goodhart-ethology"}, "j": {"title": "AGI Safety Literature Review", "author": "Tom Everitt, Gary Lea, Marcus Hutter", "date": "2018-05-03", "url": "http://arxiv.org/abs/1805.01109v2"}, "k": {"title": "Cooperative Inverse Reinforcement Learning", "author": "Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, Stuart Russell", "date": "", "url": ""}, "l": {"title": "Multi-agent Inverse Reinforcement Learning for Certain General-sum Stochastic Games", "author": "Xiaomin Lin, Stephen C. Adams, Peter A. Beling", "date": "2018-06-26", "url": "http://arxiv.org/abs/1806.09795v3"}, "m": {"title": "Towards Interactive Inverse Reinforcement Learning", "author": "Stuart Armstrong, Jan Leike", "date": "", "url": ""}, "n": {"title": "On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference", "author": "Rohin Shah, Noah Gundotra, Pieter Abbeel, Anca D. Dragan", "date": "2019-06-23", "url": "http://arxiv.org/abs/1906.09624v1"}, "o": {"title": "Directions and desiderata for AI alignment", "author": "paulfchristiano", "date": "2019-01-13", "url": "https://www.lesswrong.com/posts/kphJvksj5TndGapuh/directions-and-desiderata-for-ai-alignment"}}}