{"response": "Reinforcement Learning from Human Feedback (RLHF) is a method used in AI alignment. At a high level, RLHF learns a reward model for a certain task based on human feedback and then trains a policy to optimize the reward received from the reward model. In practice, the reward model learned is likely overfit, so the policy can benefit from interpolating between a policy that optimizes the reward model\u2019s reward and a policy trained through pure imitation learning. A key advantage of RLHF is the ease of gathering feedback and the sample efficiency required to train the reward model. For many tasks, it\u2019s significantly easier to provide feedback on a model\u2019s performance rather than attempting to teach the model through imitation [a]. \n\nRLHF is also an important building block in several proposals to align superhuman systems [d]. It has been used in tasks such as summarizing books, which exhibits some of the difficulties of aligning superhuman AI systems as it is difficult for humans to evaluate directly [d]. \n\nHowever, RLHF also has its challenges. For instance, it relies upon humans being able to evaluate the outputs of models, which may be impossible for the kinds of tasks we want to scale AI to perform [a]. It's also worth noting that while RLHF is a popular approach in labs such as OpenAI and Anthropic, there are differing opinions on its effectiveness and potential for aligning stronger AI systems [b].", "citations": {"a": {"title": "RLHF", "author": "Ansh Radhakrishnan", "date": "2022-05-12", "url": "https://www.lesswrong.com/posts/rQH4gRmPMJyjtMpTn/rlhf"}, "b": {"title": "Connor Leahy _ Promising Paths to Alignment-by AISS-video_id G4NcHrCz8yE-date 20", "author": "Connor Leahy", "date": "", "url": ""}, "c": {"title": "The MineRL BASALT Competition on Learning from Human Feedback", "author": "Rohin Shah, Cody Wild, Steven H. Wang, Neel Alex, Brandon Houghton, William Guss, Sharada Mohanty, A", "date": "2021-07-05", "url": "http://arxiv.org/abs/2107.01969v1"}, "d": {"title": "Training language models to follow instructions with human feedback", "author": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, S", "date": "2022-03-04", "url": "http://arxiv.org/abs/2203.02155v1"}, "e": {"title": "RL with KL penalties is better seen as Bayesian inference", "author": "Tomek Korbak", "date": "2022-05-25", "url": "https://www.lesswrong.com/posts/eoHbneGvqDu25Hasc/rl-with-kl-penalties-is-better-seen-as-bayesian-inference"}, "f": {"title": "The EMPATHIC Framework for Task Learning from Implicit Human Feedback", "author": "Yuchen Cui, Qiping Zhang, Alessandro Allievi, Peter Stone, Scott Niekum, W. Bradley Knox", "date": "2020-09-28", "url": "http://arxiv.org/abs/2009.13649v3"}, "g": {"title": "Room For More Funding In AI Safety Is Highly Uncertain", "author": "Evan_Gaensbauer", "date": "2016-05-12", "url": "https://www.lesswrong.com/posts/tAHYYvodamCMA9H38/room-for-more-funding-in-ai-safety-is-highly-uncertain"}}}