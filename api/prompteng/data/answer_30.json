{"response": "Inner and Outer alignment are two distinct aspects of the AI alignment problem.\n\nOuter alignment refers to the process of designing a training objective or utility function that aligns with human values or intentions. It's about ensuring that the specified loss function or objective function used to train the AI system incentivizes or produces the behavior we actually want from the AI system [a, b, c, d, i, j, k]. \n\nOn the other hand, Inner alignment is concerned with ensuring that the AI system's actual objective or behavior aligns with the specified training objective. It's about aligning the mesa-objective of a mesa-optimizer (an AI system that implements its own optimization process) with the specified loss function. It's about ensuring that the AI system generalizes 'correctly' from the training objective function and doesn't develop adversarial or harmful behaviors that were not intended [a, b, c, d, f, g, i, j].\n\nThe distinction between these two types of alignment can be understood in terms of the relationship between the AI system and its environment. Outer alignment is about the alignment between the system and the humans outside of it (specifically between the base objective and the programmer's intentions), while inner alignment is an alignment problem entirely internal to the machine learning system [b].\n\nHowever, it's important to note that the distinction between inner and outer alignment can sometimes be blurred, as many inner alignment problems can be transformed into outer alignment problems by including some of the inputs of the test into the test specification itself [a, e].", "citations": {"a": {"title": "On inner and outer alignment, and their confusion", "author": "NinaR", "date": "2022-05-26", "url": "https://www.lesswrong.com/posts/fkNEHeJja2B9yRLeT/on-inner-and-outer-alignment-and-their-confusion"}, "b": {"title": "Risks from Learned Optimization in Advanced Machine Learning Systems", "author": "Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, Scott Garrabrant", "date": "2019-06-05", "url": "http://arxiv.org/abs/1906.01820v3"}, "c": {"title": "Distinguishing AI takeover scenarios", "author": "Sam Clarke", "date": "2021-09-08", "url": "https://www.lesswrong.com/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios"}, "d": {"title": "Evan Hubinger on Inner Alignment, Outer Alignment, and Proposals for Building Sa", "author": "Evan Hubinger, Lucas Perry", "date": "", "url": ""}, "e": {"title": "\"Inner Alignment Failures\" Which Are Actually Outer Alignment Failures", "author": "johnswentworth", "date": "2020-10-31", "url": "https://www.lesswrong.com/posts/HYERofGZE6j9Tuigi/inner-alignment-failures-which-are-actually-outer-alignment"}, "f": {"title": "Clarifying the confusion around inner alignment", "author": "Rauno Arike", "date": "2022-05-13", "url": "https://www.lesswrong.com/posts/xdtNd8xCdzpgfnGme/clarifying-the-confusion-around-inner-alignment"}, "g": {"title": "[Intro to brain-like-AGI safety] 10. The alignment problem", "author": "Steven Byrnes", "date": "2022-03-30", "url": "https://www.lesswrong.com/posts/wucncPjud27mLWZzQ/intro-to-brain-like-agi-safety-10-the-alignment-problem"}, "h": {"title": "Theoretical Neuroscience For Alignment Theory", "author": "Cameron Berg", "date": "2021-12-07", "url": "https://www.lesswrong.com/posts/ZJY3eotLdfBPCLP3z/theoretical-neuroscience-for-alignment-theory"}, "i": {"title": "Interpretability\u2019s Alignment-Solving Potential: Analysis of 7 Scenarios", "author": "Evan R. Murphy", "date": "2022-05-12", "url": "https://www.lesswrong.com/posts/FrFZjkdRsmsbnQEm8/interpretability-s-alignment-solving-potential-analysis-of-7"}, "j": {"title": "A guide to Iterated Amplification & Debate", "author": "Rafael Harth", "date": "2020-11-15", "url": "https://www.lesswrong.com/posts/vhfATmAoJcN8RqGg6/a-guide-to-iterated-amplification-and-debate"}, "k": {"title": "Rohin Shah on the State of AGI Safety Research in 2021-by Future of Life Institu", "author": "Lucas Perry, Rohin Shah", "date": "", "url": ""}}}