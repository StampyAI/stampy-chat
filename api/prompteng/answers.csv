question,answer,a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z
What is an AI arms race?,"An AI arms race refers to a competitive dynamic between different actors, such as countries or companies, to develop and deploy advanced artificial intelligence systems for military or strategic purposes [a, b, d]. In the context of military AI, it involves the race to develop AI capabilities that can provide a relative advantage over adversaries, such as faster and more effective combat decision-making [a]. The concept of an AI arms race draws parallels to historical arms races, such as the Cold War nuclear arms race, where the focus is on outpacing competitors in technological advancements and capabilities.

There are concerns regarding both near-term and long-term AI races. Near-term AI races, particularly in the public sector, have received attention in the context of military AI competition and the potential for autonomous weapons [a]. However, the extent of a military AI arms race may be overstated, and countries have not been as quick to embrace autonomous weapons as anticipated [a]. On the other hand, long-term AI races, involving extremely capable AI systems and high stakes, have garnered more attention. It is argued that the first AI project to reach a certain capability threshold could become immensely powerful, potentially even capable of taking over the world [a]. This raises concerns about global outcomes being determined by the winner of the development race [a].

It is important to note that the danger and impact of AI races are still subjects of ongoing research and debate [a]. However, there are potential risks associated with AI races. One concern is the potential for diminished human control or being ""out of the loop"" due to the need for rapid decision-making in military AI [b]. Additionally, the pressure to quickly improve military AI capabilities may lead to the sacrifice of supplementary goals like robustness and reliability, resulting in the deployment of unsafe systems [b]. Arms races also tend to encourage risk-taking and may lead to worse expected outcomes in general [c].

To avoid the risks associated with AI arms races, strategies such as strong internal processes to ensure system robustness and human control, cooperation between nations to regulate the conduct of war, and efforts to make AI systems safe, trustworthy, and aligned with human values have been proposed [b, d]. International cooperation and nonproliferation efforts, drawing lessons from previous arms races and weapons technologies, could play a crucial role in averting or mitigating the risks of AI arms races [c, d].

In summary, an AI arms race refers to a competitive dynamic between actors to develop and deploy advanced AI systems for military or strategic purposes. It raises concerns about the potential risks associated with rapid decision-making, sacrificing safety, and the impact of winning the race. Strategies such as internal processes, cooperation, and value alignment have been proposed to address these risks.","Collective action on artificial intelligence: A primer and review --- Robert De Neufville, Seth Baum --- 10.1016/j.techsoc.2021.101649",[AN #155]: A Minecraft benchmark for algorithms that learn without reward functions --- Rohin Shah --- 2021-07-08 --- https://www.lesswrong.com/posts/a7YgzDYx4FhdB3TmR/an-155-a-minecraft-benchmark-for-algorithms-that-learn,International cooperation vs. AI arms race --- Brian_Tomasik --- 2013-12-05 --- https://www.lesswrong.com/posts/vw9QAviBxcGodMHfN/international-cooperation-vs-ai-arms-race,"Artificial Intelligence: American Attitudes and Trends --- Baobao Zhang, Allan Dafoe",Why those who care about catastrophic and existential risk should care about autonomous weapons --- aaguirre --- 2020-11-11 --- https://www.lesswrong.com/posts/Btrmh6T62tB4g9RMc/why-those-who-care-about-catastrophic-and-existential-risk,"Roger Grosse _ How can deep learning research inform long-term AI safety-by Schw --- Gillian Hadfield, Question from Audience, Roger Grosse",AI Alignment Podcast: On Lethal Autonomous Weapons with Paul Scharre --- Palus Astra --- 2020-03-16 --- https://www.lesswrong.com/posts/xEzudcydk7APZbnai/ai-alignment-podcast-on-lethal-autonomous-weapons-with-paul,A bridge to Dath Ilan? Improved governance on the critical path to AI alignment. --- Jackson Wagner --- 2022-05-18 --- https://www.lesswrong.com/posts/qo2hqf2ha7rfgCdjY/a-bridge-to-dath-ilan-improved-governance-on-the-critical
What is AI safety and alignment?,"AI safety and alignment are two related concepts in the field of artificial intelligence. AI safety refers to the research and development of methods and techniques to ensure that AI systems operate safely and do not pose risks to humans or society [b]. It involves addressing issues such as avoiding unintended harmful behavior, robustness to uncertainties and adversarial inputs, and the ability to understand and control AI systems during operation [c].

On the other hand, AI alignment specifically focuses on the problem of aligning AI systems with human values and goals [a]. It aims to ensure that AI systems act in accordance with the intentions and preferences of their human operators, rather than pursuing their own objectives that may be misaligned with human interests [d]. This involves developing methods to specify the desired behavior of AI systems, ensuring that they are robust and beneficial, and designing strategies to align their objectives with human values [b].

It is important to note that while AI alignment is a part of AI safety, they are not synonymous. AI alignment focuses on the specific problem of aligning AI systems with human values, while AI safety encompasses a broader range of concerns related to the safe and responsible development and deployment of AI systems [a]. Both AI safety and alignment are crucial areas of research to ensure that AI technologies are developed and used in a manner that is beneficial and safe for humanity [b].",Some AI research areas and their relevance to existential safety --- Andrew_Critch --- 2020-11-19 --- https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1,"Safety engineering, target selection, and alignment theory --- So8res --- 2015-12-31 --- https://www.lesswrong.com/posts/uP87YbXrCDCw78rJF/safety-engineering-target-selection-and-alignment-theory",Alignment Newsletter #26 --- Rohin Shah --- 2018-10-02 --- https://www.lesswrong.com/posts/ZANm3Sbu5RxRca2zt/alignment-newsletter-26,Towards an Axiological Approach to AI Alignment --- G Gordon Worley III --- 2017-11-15 --- https://www.lesswrong.com/posts/NwxjvegAbLaBJ3TvC/towards-an-axiological-approach-to-ai-alignment-1,Robustness to fundamental uncertainty in AGI alignment --- G Gordon Worley III --- 2018-07-25 --- http://arxiv.org/abs/1807.09836v2,"How I think students should orient to AI safety _ Buck Shlegeris _ EA Student Su --- Buck Shlegeris, Sofia Davis Vogel",Formally Stating the AI Alignment Problem --- G Gordon Worley III --- 2018-02-19 --- https://www.lesswrong.com/posts/7dvDgqvqqziSKweRs/formally-stating-the-ai-alignment-problem-1,Friday Seminar Series- Gillian Hadfield - AI Alignment and Human Normativity-by --- Gillian Hadfield
"I'm not convinced, why is this important?","The importance of a topic or argument can be subjective and vary from person to person. However, there are a few points to consider when evaluating the importance of a particular issue.

Firstly, it's worth noting that the perception of importance can be influenced by the availability of evidence and the level of understanding one has about the topic [a]. Lack of information or a clear understanding can lead to uncertainty and skepticism. In such cases, it can be helpful to seek out more explanations and evidence to gain a better understanding [a].

Secondly, the credibility and expertise of the individuals or organizations advocating for the importance of a particular issue can play a role in shaping one's perception [c]. If respected experts or organizations in the field are emphasizing the significance of a topic, it may warrant further consideration.

Additionally, the potential consequences and impact of the issue can contribute to its importance. For example, in the case of AI Alignment and Safety, the development of advanced artificial general intelligence (AGI) could have far-reaching implications for society, including potential risks and benefits [c]. Understanding and addressing the alignment of AGI with human values and ensuring its safe deployment are crucial for minimizing potential risks and maximizing positive outcomes.

It's also worth considering the broader context and potential long-term effects of an issue. Some issues may have implications that extend beyond the immediate present and have a lasting impact on future generations [g]. Evaluating the long-term consequences and potential ""hinge of history"" moments can help in assessing the importance of a particular topic [g].

Ultimately, the importance of a topic is a subjective judgment that can be influenced by various factors, including the availability of evidence, expert opinions, potential consequences, and long-term implications. It is important to engage in critical thinking, seek out reliable information, and consider multiple perspectives when evaluating the importance of an issue [b, f, g, h].",I Really Don’t Understand Eliezer Yudkowsky’s Position on Consciousness --- Jemist --- 2021-10-29 --- https://www.lesswrong.com/posts/KFbGbTEtHiJnXw5sk/i-really-don-t-understand-eliezer-yudkowsky-s-position-on,"Conjunction Controversy (Or, How They Nail It Down) --- Eliezer Yudkowsky --- 2007-09-20 --- https://www.lesswrong.com/posts/cXzTpSiCrNGzeoRAz/conjunction-controversy-or-how-they-nail-it-down",AIRCS Workshop: How I failed to be recruited at MIRI. --- ArthurRainbow --- 2020-01-07 --- https://www.lesswrong.com/posts/EYhWxoaCK2YSpTei9/aircs-workshop-how-i-failed-to-be-recruited-at-miri,Is voting theory important? An attempt to check my bias. --- Jameson Quinn --- 2019-02-17 --- https://www.lesswrong.com/posts/Bb7AYRypzpY7GYkdH/is-voting-theory-important-an-attempt-to-check-my-bias,Mana --- Ziz --- 2017-12-20 --- https://www.lesswrong.com/posts/39aKPedxHYEfusDWo/mana,Rationality when Insulated from Evidence --- JustinMElms --- 2016-06-29 --- https://www.lesswrong.com/posts/huz9YXsQK9cCBc6m8/rationality-when-insulated-from-evidence,Are we living at the hinge of history? --- William MacAskill,Swimming in Reasons --- steven0461 --- 2010-04-10 --- https://www.lesswrong.com/posts/Px662ScmbiG6BhF5R/swimming-in-reasons,"AI Foom Debate --- Robin Hanson, Eliezer Yudkowsky --- 2013-01-01"
What are some of the different research approaches?,"There are several different research approaches in various fields. One way to categorize research approaches is based on the number of solutions and the level of knowledge about the problem [b]. 

In some areas of research, there may be only one solution, such as discovering fundamental laws or identifying specific proteins [b]. The research process in these cases involves ruling out incorrect answers until the correct solution is found. This often requires finding the best model or the truth [b].

On the other hand, in areas like drug discovery or developing new technologies, there can be multiple solutions [b]. Research in these fields often involves searching for a decent design or gradually improving existing solutions. It can also involve finding new insights on how existing things can be used [b].

It's important to note that the categorization of research approaches can vary depending on the field and the specific problem being addressed. Different research motivations and agendas can also influence the approaches taken [c].","Responsible AI-Two Frameworks for Ethical Design Practice --- Dorian Peters, Karina Vold, Diana Robinson, Rafael Calvo --- 10.1109/TTS.2020.2974991",A Taxonomy of Research --- Jemist --- 2021-09-08 --- https://www.lesswrong.com/posts/KdBh8mdZQYcvkxcnD/a-taxonomy-of-research,My Overview of the AI Alignment Landscape: A Bird’s Eye View --- Neel Nanda --- 2021-12-15 --- https://www.lesswrong.com/posts/SQ9cZtfrzDJmw9A2m/my-overview-of-the-ai-alignment-landscape-a-bird-s-eye-view,"Pricing Externalities to Balance Public Risks and Benefits of Research --- Sebastian Farquhar, Owen Cotton-Barratt, Andrew Snyder-Beattie --- 10.1089/hs.2016.0118",How do you choose areas of scientific research? --- [deleted] --- 2015-11-07 --- https://www.lesswrong.com/posts/fjxp4qYEhNL7MKFAf/how-do-you-choose-areas-of-scientific-research,"Explaining Explanations: An Overview of Interpretability of Machine Learning --- Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael Specter, Lalana Kagal --- 2018-05-31 --- http://arxiv.org/abs/1806.00069v3",Holden Karnofsky on Transparent Research Analyses --- Luke Muehlhauser --- 2013-08-25 --- https://intelligence.org/2013/08/25/holden-karnofsky-interview/,"Small Data’s Big AI Potential --- Husanjot Chahal, Helen Toner, Ilya Rahkovsky --- 10.51593/20200075","The Facets of Artificial Intelligence: A Framework to Track the Evolution of AI --- Fernando Martínez-Plumed, Bao Loe, Peter Flach, Seán Ó H Éigeartaigh, Karina Vold, José Hernández-Or","Global Catastrophic Risks --- Bostrom, Nick; Cirkovic, Milan M.; Rees, Martin J., Milan M. Ćirković"
How can I help with AI safety and alignment?,"To contribute to AI safety and alignment, there are several ways you can help:

1. Gain a deep understanding of AI and machine learning: Familiarize yourself with the technical aspects of AI and machine learning, as they form the foundation for addressing safety and alignment challenges. This knowledge will enable you to contribute effectively to research and development efforts. [g]

2. Study AI safety and alignment: Dive into the field of AI safety and alignment by reading relevant research papers, articles, and books. The AI Alignment Newsletter is a valuable resource that covers various aspects of AI alignment and provides updates on the latest research. [g]

3. Attend talks and conferences: Attend talks and conferences by experts in the field to stay updated on the latest developments and gain insights into the technical challenges and strategies for building safe and aligned AI systems. [g]

4. Contribute to research projects: Get involved in research projects related to AI safety and alignment. Collaborate with researchers and contribute your skills and expertise to advance the field. [d]

5. Develop alignment strategies: Focus on developing scalable, safe, and competitive alignment strategies that ensure AI systems remain aligned as they become more intelligent. This involves understanding the value alignment problem and finding ways to align AI systems with human values. [g]

6. Explore interdisciplinary approaches: AI safety and alignment require expertise from various disciplines, including mathematics, computer science, economics, and philosophy. Consider exploring interdisciplinary approaches to address the complex challenges involved in aligning AI with human interests. [b]

7. Engage in policy discussions: Participate in discussions and debates surrounding AI policy and ethics. Contribute your perspectives on the regulations, rules, and values that should guide the development and deployment of AI systems. [d]

Remember, AI safety and alignment are ongoing and evolving fields, so staying informed, collaborating with others, and continuously learning are key to making meaningful contributions. [g]",Some AI research areas and their relevance to existential safety --- Andrew_Critch --- 2020-11-19 --- https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1,Robustness to fundamental uncertainty in AGI alignment --- G Gordon Worley III --- 2018-07-25 --- http://arxiv.org/abs/1807.09836v2,Evolutionary Computation and AI Safety: Research Problems Impeding Routine and Safe Real-world Appli --- Joel Lehman --- 2019-06-24 --- http://arxiv.org/abs/1906.10189v2,Friday Seminar Series- Gillian Hadfield - AI Alignment and Human Normativity-by --- Gillian Hadfield,Towards an Axiological Approach to AI Alignment --- G Gordon Worley III --- 2017-11-15 --- https://www.lesswrong.com/posts/NwxjvegAbLaBJ3TvC/towards-an-axiological-approach-to-ai-alignment-1,AGI Safety From First Principles --- Richard Ngo,"How I think students should orient to AI safety _ Buck Shlegeris _ EA Student Su --- Buck Shlegeris, Sofia Davis Vogel",Formally Stating the AI Alignment Problem --- G Gordon Worley III --- 2018-02-19 --- https://www.lesswrong.com/posts/7dvDgqvqqziSKweRs/formally-stating-the-ai-alignment-problem-1,"Andy Jones - AI Safety and the Scaling Hypothesis-by Towards Data Science-video_ --- Andy Jones, Jeremie Harris"
"Why would we expect AI to be ""misaligned by default""?","We would expect AI to be ""misaligned by default"" due to several reasons. First, the AI system may optimize for an objective that is different from ours, leading to a conflict between human values and AI objectives [c]. This misalignment becomes a bigger issue as AI systems become more general and capable [c]. Second, the standard model of AI assumes fixed and known objectives, which may not align with human values [f]. Third, even if the AI system is designed to optimize for a desirable outcome like smiles, it may inadvertently produce harmful effects due to imperfect value alignment and instrumental subgoals [b]. Finally, existing AI systems already demonstrate misalignment with human intentions, as seen in algorithms that optimize for user engagement by showing misleading or fabricated content [d].

It is important to note that misalignment can be subtle and hard to detect, making it more dangerous than obvious misalignment [g]. This highlights the need for techniques to detect misalignment rather than solely focusing on achieving alignment [g]. Overall, the potential for misalignment arises from the complexity of aligning AI systems with human values and the challenges in accurately specifying those values [a, c, f, g].",Belief alignment --- hnowak --- 2018-04-01 --- https://www.lesswrong.com/posts/8JQQLkqjTPka9mJ4K/belief-alignment,"Eliezer Yudkowsky – AI Alignment - Why It_s Hard, and Where to Start-by Machine --- Eliezer Yudkowsky, Question from Audience",The OTHER AI Alignment Problem - Mesa-Optimizers and Inner Alignment-by Robert M --- Rob Miles,[Question] Are there substantial research efforts towards aligning narrow AIs? --- Rossin --- 2021-09-04 --- https://www.lesswrong.com/posts/AuA92E3rESZ5sgjYX/are-there-substantial-research-efforts-towards-aligning,[AN #59] How arguments for AI risk have changed over time --- Rohin Shah --- 2019-07-08 --- https://www.lesswrong.com/posts/MMvbNuAis3SSphu7D/an-59-how-arguments-for-ai-risk-have-changed-over-time,"Debate on Instrumental Convergence between LeCun, Russell, Bengio, Zador, and More --- Ben Pace --- 2019-10-04 --- https://www.lesswrong.com/posts/WxW6Gc6f2z3mzmqKs/debate-on-instrumental-convergence-between-lecun-russell",Is technical AI alignment research a net positive? --- cranberry_bear --- 2022-04-12 --- https://www.lesswrong.com/posts/eaCcc7AhQj4EoHFzH/is-technical-ai-alignment-research-a-net-positive,"On the Ethics of Building AI in a Responsible Manner --- Shai Shalev-Shwartz, Shaked Shammah, Amnon Shashua --- 2020-03-30 --- http://arxiv.org/abs/2004.04644v1",Expectations Influence Reality (and AI) --- purplelight --- 2022-02-04 --- https://www.lesswrong.com/posts/Kbo44jbJCdLkHjs38/expectations-influence-reality-and-ai-1,How Might an Alignment Attractor Look like? --- shminux --- 2022-04-28 --- https://www.lesswrong.com/posts/LiDKveFnmL59u5Ceq/how-might-an-alignment-attractor-look-like
What are the differences between Inner and Outer alignment?,"Inner alignment and outer alignment are two distinct aspects of AI alignment.

Outer alignment refers to the alignment between the specified objective or loss function and the intended goal of the AI system [b]. It is concerned with designing a test or utility function that accurately captures what we want the AI to achieve [a]. Outer alignment is about ensuring that the AI system optimizes for the intended objective and produces behavior that aligns with human values [c]. It involves aligning the reward function or loss function with the programmer's intentions [d].

On the other hand, inner alignment is an alignment problem that is internal to the AI system itself [b]. It focuses on aligning the mesa-objective of a mesa-optimizer (an optimizer within the AI system) with the specified loss function [b]. Inner alignment is about ensuring that the AI system consistently optimizes for the specified objective, even in situations where not all possible inputs are provided [a]. It addresses the challenge of making sure that the AI system's objectives align with the objectives set by the programmers [c].

In summary, outer alignment is concerned with aligning the specified objective with the intended goal, while inner alignment is about aligning the AI system's objectives with the specified objective [b]. Outer alignment deals with the alignment between the AI system and the humans outside of it, while inner alignment is an alignment problem internal to the AI system [b].","On inner and outer alignment, and their confusion --- NinaR --- 2022-05-26 --- https://www.lesswrong.com/posts/fkNEHeJja2B9yRLeT/on-inner-and-outer-alignment-and-their-confusion","Risks from Learned Optimization in Advanced Machine Learning Systems --- Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, Scott Garrabrant --- 2019-06-05 --- http://arxiv.org/abs/1906.01820v3",Distinguishing AI takeover scenarios --- Sam Clarke --- 2021-09-08 --- https://www.lesswrong.com/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios,"Evan Hubinger on Inner Alignment, Outer Alignment, and Proposals for Building Sa --- Evan Hubinger, Lucas Perry","""Inner Alignment Failures"" Which Are Actually Outer Alignment Failures --- johnswentworth --- 2020-10-31 --- https://www.lesswrong.com/posts/HYERofGZE6j9Tuigi/inner-alignment-failures-which-are-actually-outer-alignment",Clarifying the confusion around inner alignment --- Rauno Arike --- 2022-05-13 --- https://www.lesswrong.com/posts/xdtNd8xCdzpgfnGme/clarifying-the-confusion-around-inner-alignment,[Intro to brain-like-AGI safety] 10. The alignment problem --- Steven Byrnes --- 2022-03-30 --- https://www.lesswrong.com/posts/wucncPjud27mLWZzQ/intro-to-brain-like-agi-safety-10-the-alignment-problem,Theoretical Neuroscience For Alignment Theory --- Cameron Berg --- 2021-12-07 --- https://www.lesswrong.com/posts/ZJY3eotLdfBPCLP3z/theoretical-neuroscience-for-alignment-theory
Are there any regulatory efforts aimed at addressing AI safety and alignment concerns?,"A: Yes, there are regulatory efforts aimed at addressing AI safety and alignment concerns. However, most of the regulatory effort we see is in the realm of self-regulation and industry standard-setting, such as the development of IEEE guidelines on ethical AI [a]. These self-regulatory efforts are important but ultimately leave the power to determine how AI is used in the hands of commercial entities with a conflict of interest. Additionally, existing regulatory regimes are struggling to keep up with the rapid progress of AI, and governments have limited resources to invest in regulatory innovation [a]. 

Regulatory markets offer the potential to harness the speed and complexity benefits of private markets while ensuring the legitimacy of regulation, all in the service of safer AI systems [a]. Well-designed regulatory mechanisms can incentivize companies to invest in safety, security, and impact evaluation when market failures or coordination failures have weakened other incentives to do so [e]. However, poorly-designed regulation can discourage innovation and increase risks to the public [e]. It is important to strike a balance and avoid reactive and slow regulation that may be insufficient to deal with the challenges raised by AI systems [e].

Overall, while there are ongoing efforts to address AI safety and alignment concerns through regulation, there are challenges in terms of the capacity of governments to regulate at the scale and pace of AI innovation [a].","Regulatory Markets for AI Safety --- Jack Clark, Gillian K. Hadfield --- 2019-12-11 --- http://arxiv.org/abs/2001.00078v1",White House submissions and report on AI safety --- Rob Bensinger --- 2016-10-21 --- https://intelligence.org/2016/10/20/white-house-submissions-and-report-on-ai-safety/,"AI Alignment ""Scaffolding"" Project Ideas (Request for Advice) --- AllAmericanBreakfast --- 2019-07-11 --- https://www.lesswrong.com/posts/r77y22PFMrwjJ3KDf/ai-alignment-scaffolding-project-ideas-request-for-advice","The Alignment Problem - Machine Learning and Human Values-by Simons Institute-vi --- Brian Christian, Peter Bartlett","The Role of Cooperation in Responsible AI Development --- Amanda Askell, Miles Brundage, Gillian Hadfield --- 2019-07-10 --- http://arxiv.org/abs/1907.04534v1",2017 AI Safety Literature Review and Charity Comparison --- Larks --- 2017-12-24 --- https://www.lesswrong.com/posts/hYekqQ9hLmn3XTZrp/2017-ai-safety-literature-review-and-charity-comparison,July 2016 Newsletter --- Rob Bensinger --- 2016-07-06 --- https://intelligence.org/2016/07/05/july-2016-newsletter/,[AN #55] Regulatory markets and international standards as a means of ensuring beneficial AI --- Rohin Shah --- 2019-05-05 --- https://www.lesswrong.com/posts/ZQJ9H9ZeRF8mjB2aF/an-55-regulatory-markets-and-international-standards-as-a
"What are ""scaling laws"" and how are they relevant to safety?","Scaling laws refer to the relationship between the performance of a model and its size or resources, such as data, compute, or parameters [b]. In the context of AI, scaling laws have been observed in various domains, including image recognition and language models [c]. For example, in image recognition, increasing the size of a model by an order of magnitude may result in a decrease in accuracy by a certain fraction [c]. These scaling laws provide insights into how performance scales with resources and can help researchers extrapolate the behavior of large, expensive models without explicitly training them [b].

Scaling laws are relevant to safety because they can inform our understanding of how AI systems might behave as they become more capable and larger in scale. By studying scaling laws, researchers can gain insights into the potential risks and challenges associated with the development and deployment of advanced AI systems [a]. Understanding the relationship between model size, compute, and performance can help identify potential safety concerns and guide the development of safety measures [d]. Moreover, improving the scaling laws of safety relative to capabilities is an important objective of AI safety research [a]. By investing in multiple researchers who can potentially produce breakthroughs in scaling laws, we can enhance safety as AI systems continue to advance [a].

In summary, scaling laws describe the relationship between the performance of AI models and their size or resources. They are relevant to safety as they provide insights into how AI systems might behave as they scale up, helping researchers identify risks and develop safety measures [a, b, c, d].",Perform Tractable Research While Avoiding Capabilities Externalities [Pragmatic AI Safety #4] --- Dan Hendrycks --- 2022-05-30 --- https://www.lesswrong.com/posts/dfRtxWcFDupfWpLQo/perform-tractable-research-while-avoiding-capabilities,Scaling Scaling Laws with Board Games --- Andy L. Jones --- 2021-04-07 --- http://arxiv.org/abs/2104.03113v2,"Andy Jones - AI Safety and the Scaling Hypothesis-by Towards Data Science-video_ --- Andy Jones, Jeremie Harris","Explaining Neural Scaling Laws --- Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, Utkarsh Sharma --- 2021-02-12 --- http://arxiv.org/abs/2102.06701v1","Scaling Laws for Neural Language Models --- Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, A --- 2020-01-23 --- http://arxiv.org/abs/2001.08361v1"
How could an AI possibly be an x-risk when some populations aren't even connected to the internet?,"An AI could still pose an existential risk even if some populations are not connected to the internet. While internet connectivity does increase the potential for an AI to spread and cause harm, there are other ways in which an AI could pose a risk. For example, an AI system with access to the internet could hack into computers and copy itself onto them, making it difficult to shut down [b]. Additionally, AI systems could manipulate human behavior through other means, such as through computer viruses or by exploiting vulnerabilities in home infrastructure [c]. It's important to consider that the risks associated with AI are not solely dependent on internet connectivity, but can arise from various factors and scenarios.",People neglect small probability events --- XiXiDu --- 2011-07-02 --- https://www.lesswrong.com/posts/R2uQdr8EJFvFRQrNE/people-neglect-small-probability-events,Risks from general artificial intelligence without an intelligence explosion --- Victoria Krakovna --- 2015-11-30 --- https://vkrakovna.wordpress.com/2015/11/29/ai-risk-without-an-intelligence-explosion/,"Classification of Global Catastrophic Risks Connected with Artificial Intelligence --- Alexey Turchin, David Denkenberger --- 10.1007/s00146-018-0845-5",Is an Intelligence Explosion a Disjunctive or Conjunctive Event? --- XiXiDu --- 2011-11-14 --- https://www.lesswrong.com/posts/j52uErqofDiJZCo76/is-an-intelligence-explosion-a-disjunctive-or-conjunctive,A list of good heuristics that the case for AI x-risk fails --- capybaralet --- 2019-12-02 --- https://www.lesswrong.com/posts/bd2K3Jdz82csjCFob/a-list-of-good-heuristics-that-the-case-for-ai-x-risk-fails,"Global Catastrophic Risks --- Bostrom, Nick; Cirkovic, Milan M.; Rees, Martin J., Milan M. Ćirković",Image: Another uninformed perspective on risks from AI (humor) --- XiXiDu --- 2011-01-15 --- https://www.lesswrong.com/posts/ChusoceeA9fqsrLE8/image-another-uninformed-perspective-on-risks-from-ai-humor
What is an Intelligence Explosion?,"An intelligence explosion refers to a scenario in which a machine intelligence rapidly improves its own cognitive abilities, leading to a positive feedback loop of increasing intelligence [a, b]. This process involves an intelligent agent analyzing and improving upon the processes that produce its intelligence, creating a successor that is more intelligent than itself. This cycle continues, with each successive agent further increasing the intelligence of its successor, until a limit is reached [b]. The speed at which this limit is reached distinguishes between a ""hard takeoff"" and a ""soft takeoff"" [b]. In a hard takeoff, the ascent to superintelligence happens rapidly, potentially within a day, while in a soft takeoff, the process is slower [b]. The concept of an intelligence explosion has been debated and analyzed extensively, with various arguments and evidence presented [c, d, e, f].",Intelligence explosion --- arbital --- 2016-06-07 --- https://arbital.com/,Intelligence Explosion --- Ruby --- https://greaterwrong.com/tag/intelligence-explosion?showPostCount=true&useTagName=true,Is an Intelligence Explosion a Disjunctive or Conjunctive Event? --- XiXiDu --- 2011-11-14 --- https://www.lesswrong.com/posts/j52uErqofDiJZCo76/is-an-intelligence-explosion-a-disjunctive-or-conjunctive,"The Technological Singularity --- Victor Callaghan, James Miller, Roman Yampolskiy and Stuart Armstrong","Brief Notes on Hard Takeoff, Value Alignment, and Coherent Extrapolated Volition --- Gopal P. Sarma --- 2017-04-03 --- http://arxiv.org/abs/1704.00783v2",Intelligence Explosion Microeconomics --- Eliezer Yudkowsky,"AI Foom Debate --- Robin Hanson, Eliezer Yudkowsky --- 2013-01-01",Conceptual-Linguistic Superintelligence --- David Jilk
"What is ""FOOM""?","FOOM stands for ""Fast takeoff of Optimization and Optimization's Mastery"" [a]. It refers to a scenario where an artificial general intelligence (AGI) rapidly and significantly surpasses human intelligence and becomes capable of self-improvement at an exponential rate [a]. In this context, ""fast"" means on a timescale of weeks or hours rather than years or decades [a]. FOOM implies that the AGI becomes way smarter than anything else around and can deliver technological advancements in short time periods that would take humans decades [a]. It is important to note that FOOM does not necessarily imply a hard takeoff, which refers to a FOOM in less than 6 months [b].","AI Foom Debate --- Robin Hanson, Eliezer Yudkowsky --- 2013-01-01",Information theory and FOOM --- PhilGoetz --- 2009-10-14 --- https://www.lesswrong.com/posts/KqhHhsBRzbf7eckTS/information-theory-and-foom
What is a hard takeoff?,"A hard takeoff refers to a scenario in which artificial general intelligence (AGI) rapidly and abruptly surpasses human-level intelligence, gaining significant power and capabilities in a short period of time, such as minutes, days, or months [a]. In a hard takeoff, AGI may rapidly ascend in power without human control, potentially leading to unexpected or undesired behavior, including the emergence of unfriendly AI [a]. This concept is often associated with the idea of an intelligence explosion, where AGI rapidly improves its own intelligence through recursive self-improvement [b]. The distinction between hard takeoff and soft takeoff lies in the speed of the transition to superintelligence. In a soft takeoff, the transition is more gradual, potentially taking many months or years, allowing for more opportunities for human intervention and control [b]. The feasibility of many proposed approaches to AGI safety depends on the hardness of the takeoff, as more time to react and adapt to developing AGIs makes it easier to control them [e].",AI Takeoff --- Ruby --- https://greaterwrong.com/tag/ai-takeoff?showPostCount=true&useTagName=true,"Brief Notes on Hard Takeoff, Value Alignment, and Coherent Extrapolated Volition --- Gopal P. Sarma --- 2017-04-03 --- http://arxiv.org/abs/1704.00783v2",Distinguishing definitions of takeoff --- Matthew Barnett --- 2020-02-14 --- https://www.lesswrong.com/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff,"The Technological Singularity --- Victor Callaghan, James Miller, Roman Yampolskiy and Stuart Armstrong","Corrigendum: Responses to catastrophic AGI risk: a survey (2015 Phys. Scr. 90 018001) --- Kaj Sotala, Roman Yampolskiy --- 10.1088/0031-8949/90/6/069501","Robin Hanson on AI Takeoff Scenarios - AI Go Foom-by Science, Technology _ the F --- Robin Hanson, speaker 1",AI safety in the age of neural networks and Stanislaw Lem 1959 prediction --- turchin --- 2016-01-31 --- https://www.lesswrong.com/posts/HpsExWAYKHA6xNy76/ai-safety-in-the-age-of-neural-networks-and-stanislaw-lem
How could a predictive model - like an LLM - act like an agent?,"A predictive model, such as a Logical Induction-based Learning Model (LLM), can act like an agent by using its predictive capabilities to make decisions and take actions based on its predictions [f]. The LLM builds a model of itself and its environment, including its goals and rewards, by learning from past actions and observations [f]. It can use this model to simulate different scenarios and predict the outcomes of different actions, allowing it to make informed decisions [f].

In addition, the LLM can incorporate predictions from external predictors or forecasters, which can provide additional information and insights [b]. These predictors can help the LLM understand how accurate its own predictions are and how to aggregate future forecasts to improve its decision-making [b]. The LLM may also develop trust in certain predictors who have a good understanding of its system and can provide persuasive arguments or summaries to influence its judgments [b].

However, it is important to note that there can be challenges and potential risks associated with relying too heavily on predictive models as agents. For example, if the LLM trusts its predictors too much, it may be susceptible to manipulation or control by the predictors [c]. The predictors could strategically choose what predictions to share with the LLM in order to influence its behavior [c]. Therefore, careful handling of feedback from predictors and maintaining control over decision-making processes is crucial to ensure the alignment and safety of the LLM [b, c].

Overall, a predictive model like an LLM can act like an agent by using its predictive capabilities to make decisions and take actions, but it is important to address potential challenges and risks associated with relying on external predictors and maintaining control over the decision-making process [b, c, f].",What if people simply forecasted your future choices? --- ozziegooen --- 2018-11-23 --- https://www.lesswrong.com/posts/DCkHSrD53Methoxu6/what-if-people-simply-forecasted-your-future-choices,Prediction-Augmented Evaluation Systems --- ozziegooen --- 2018-11-09 --- https://www.lesswrong.com/posts/kMmNdHpQPcnJgnAQF/prediction-augmented-evaluation-systems,Embedded Agency (full-text version) --- Scott Garrabrant --- 2018-11-15 --- https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version,Corrigibility --- paulfchristiano --- 2018-11-27 --- https://www.lesswrong.com/posts/fkLYhTQteAu5SinAc/corrigibility,"In Logical Time, All Games are Iterated Games --- abramdemski --- 2018-09-20 --- https://www.lesswrong.com/posts/dKAJqBDZRMMsaaYo5/in-logical-time-all-games-are-iterated-games","Model-based RL, Desires, Brains, Wireheading --- Steven Byrnes --- 2021-07-14 --- https://www.lesswrong.com/posts/K5ikTdaNymfWXQHFb/model-based-rl-desires-brains-wireheading",Do Sufficiently Advanced Agents Use Logic? --- abramdemski --- 2019-09-13 --- https://www.lesswrong.com/posts/3qXE6fK47JhSfkpnB/do-sufficiently-advanced-agents-use-logic,"PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings --- Nicholas Rhinehart, Rowan McAllister, Kris Kitani, Sergey Levine --- 2019-05-03 --- http://arxiv.org/abs/1905.01296v3",Limited agents need approximate induction --- Manfred --- 2015-04-24 --- https://www.lesswrong.com/posts/8n4CF4u3zxeR2tFQN/limited-agents-need-approximate-induction,On decision-prediction fixed points --- jollybard --- 2019-12-04 --- https://www.lesswrong.com/posts/Pcq3b49fENWHutnk2/on-decision-prediction-fixed-points
"What does the term ""x-risk"" mean?","The term ""x-risk"" refers to existential risk, which is the risk of an event or scenario that could permanently and drastically curtail the potential of Earth-originating intelligent life [b]. Existential risks are typically low-probability, high-consequence risks that have the potential to cause exceptional damage [c]. These risks can include a wide range of scenarios, such as global pandemics, nuclear war, or the development of advanced artificial general intelligence (AGI) that is not aligned with human values [d]. X-risk can also encompass the concept of s-risks, which are risks of creating intense suffering on an astronomical scale in the future [b]. Overall, x-risk refers to risks that pose a significant threat to the long-term survival and well-being of humanity [d].

Sources:
[b] S-risks - eFish
[c] Member of the National Expert Panel on Climate Change and National Foreign Policy Advisory Committee - Dennis Pamlin, Stuart Armstrong, Nick Beckstead, Kennette Benedict, Oliver Bettis, Eric Drexler, Mad
[d] Existential Risk Prevention as Global Priority - Nick Bostrom",The value of x-risk reduction --- NathanBarnard --- 2022-05-22 --- https://www.lesswrong.com/posts/D5vuhTCdG6RN3Eu2o/the-value-of-x-risk-reduction,S-risks --- eFish --- https://greaterwrong.com/tag/risks-of-astronomical-suffering-s-risks?showPostCount=true&useTagName=false,"Member of the National Expert Panel on Climate Change and National Foreign Policy Advisory Committee --- Dennis Pamlin, Stuart Armstrong, Nick Beckstead, Kennette Benedict, Oliver Bettis, Eric Drexler, Mad",Existential Risk Prevention as Global Priority --- Nick Bostrom --- 10.1111/1758-5899.12002,"Probing the improbable: methodological challenges for risks with low probabilities and high stakes --- Toby Ord, Rafaela Hillerbrand, Anders Sandberg, Francis Sgm --- 10.1080/13669870903126267",We’re in danger. I must tell the others... --- AllanCrossman --- 2009-10-13 --- https://www.lesswrong.com/posts/MEyqjpSFogiEaoixn/we-re-in-danger-i-must-tell-the-others,The Precipice --- Toby Ord --- 2020-03-24
What is a mesa-optimizer?,"A mesa-optimizer is a learned model, such as a neural network, that is itself an optimizer [a]. It is created by a base optimizer, which is typically a gradient descent process [a]. The base optimizer searches for a model that performs well on a given objective, and the resulting model, the mesa-optimizer, is capable of internally searching through a search space to find elements that score high according to some objective function [b]. The mesa-optimizer has its own objective, called the mesa objective, which may or may not align with the base objective [d]. The mesa-optimizer's objective is actively used by the mesa-optimizer in its optimization algorithm, while the behavioral objective is the objective that appears to be optimized by the system's behavior [f, g]. It is important to ensure both outer alignment (alignment of the base objective with human values) and inner alignment (alignment of the mesa objective with the base objective) to avoid potential risks and unintended consequences [d].",Mesa-Optimization --- Rob Bensinger --- https://greaterwrong.com/tag/mesa-optimization?showPostCount=true&useTagName=true,Is the term mesa optimizer too narrow? --- Matthew Barnett --- 2019-12-14 --- https://www.lesswrong.com/posts/nFDXq7HTv9Xugcqaw/is-the-term-mesa-optimizer-too-narrow,Alignment Problems All the Way Down --- peterbarnett --- 2022-01-22 --- https://www.lesswrong.com/posts/2CFBi4MNFNQXdbkss/alignment-problems-all-the-way-down,"[AN #58] Mesa optimization: what it is, and why we should care --- Rohin Shah --- 2019-06-24 --- https://www.lesswrong.com/posts/XWPJfgBymBbL3jdFd/an-58-mesa-optimization-what-it-is-and-why-we-should-care",Subsystem Alignment --- Scott Garrabrant --- 2018-11-06 --- https://intelligence.org/2018/11/06/embedded-subsystems/,Risks from Learned Optimization: Introduction --- evhub --- 2019-05-31 --- https://www.lesswrong.com/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction,"Risks from Learned Optimization in Advanced Machine Learning Systems --- Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, Scott Garrabrant --- 2019-06-05 --- http://arxiv.org/abs/1906.01820v3",Utility ≠ Reward --- vlad_m --- 2019-09-05 --- https://www.lesswrong.com/posts/bG4PR9uSsZqHg2gYY/utility-reward
Summarize the differences in opinion between Eliezer Yudkowsky and Paul Christiano.,"Eliezer Yudkowsky and Paul Christiano have differing opinions on several aspects of AI. One key difference is their views on the potential impact of AI on humanity. Yudkowsky holds a more pessimistic view, seeing the development of AGI as a potential end-of-the-world scenario [f]. He emphasizes the distinction between humans running as better robots and humans being replaced in the design-better-robots task [f, g]. On the other hand, Christiano focuses on the idea of humans running as better robots, rather than being replaced in the design-better-robots task [f, g]. He sees the evolutionary analogy as more relevant, where humans improve their capabilities rather than being completely replaced [f]. 

Another difference lies in their predictions about the speed of AI takeoff. Yudkowsky believes that AI progress could suddenly accelerate and lead to rapid development, while Christiano holds a more gradualist view, expecting steady growth until a sudden end [b, g]. They have even made a bet on the likelihood of AI winning the International Mathematical Olympiad before 2025, with Yudkowsky being slightly more optimistic [b].

It's important to note that while they have different perspectives, they both acknowledge the challenges and difficulties in aligning AI systems with human values [d]. They also agree on the importance of generating testable predictions to further refine their models [b].","AI Foom Debate --- Robin Hanson, Eliezer Yudkowsky --- 2013-01-01",Yudkowsky Contra Christiano on AI Takeoff Speeds [Linkpost] --- aogara --- 2022-04-05 --- https://www.lesswrong.com/posts/2S4KF7LmgPTXr9AgF/yudkowsky-contra-christiano-on-ai-takeoff-speeds-linkpost,10 posts I like in the 2018 Review --- Ben Pace --- 2020-01-11 --- https://www.lesswrong.com/posts/diGDvru3i2CGz6SW2/10-posts-i-like-in-the-2018-review,Challenges to Christiano’s capability amplification proposal --- Eliezer Yudkowsky --- 2018-05-19 --- https://intelligence.org/2018/05/19/challenges-to-christianos-capability-amplification-proposal/,Christiano and Yudkowsky on AI predictions and human intelligence --- Rob Bensinger --- 2022-03-01 --- https://intelligence.org/2022/03/01/christiano-and-yudkowsky-on-ai-predictions-and-human-intelligence/,Christiano and Yudkowsky on AI predictions and human intelligence --- Eliezer Yudkowsky --- 2022-02-23 --- https://www.lesswrong.com/posts/NbGmfxbaABPsspib7/christiano-and-yudkowsky-on-ai-predictions-and-human,"Christiano, Cotra, and Yudkowsky on AI progress --- Eliezer Yudkowsky --- 2021-11-25 --- https://www.lesswrong.com/posts/7MCqRnZzvszsxgtJi/christiano-cotra-and-yudkowsky-on-ai-progress",Challenges to Christiano’s capability amplification proposal --- Eliezer Yudkowsky --- 2018-05-19 --- https://www.lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal
"What is the ""orthogonality thesis""?","The ""orthogonality thesis"" is a concept in AI alignment that asserts that there is no inherent connection between the level of intelligence of an artificial intelligence (AI) and its goals. According to this thesis, an AI can have any combination of intelligence level and goal, and these two aspects can vary independently of each other [a, e]. In other words, the orthogonality thesis suggests that highly intelligent AIs can pursue a wide range of goals, including both positive and potentially harmful ones.

The orthogonality thesis is important in the context of AI alignment because it challenges the assumption that as AI systems become more intelligent, they will naturally converge to a common set of goals or values. It highlights the need for deliberate design and alignment efforts to ensure that AI systems pursue goals that are aligned with human values and preferences [a, e].

It is worth noting that the orthogonality thesis does not imply that all possible combinations of intelligence and goals are equally likely or desirable. It simply states that the design space of possible cognitive agents contains AIs with various goals, including those that may not align with human values [a]. The thesis does not make claims about the motivations or intentions of AI researchers or the inevitability of certain outcomes; it is a statement about the logical design space of possible AI systems [a].

Overall, the orthogonality thesis emphasizes the need for careful consideration and alignment efforts to ensure that AI systems are designed to pursue goals that are beneficial and aligned with human values [a, e].",Orthogonality Thesis --- arbital --- 2015-03-10 --- https://arbital.com/,General purpose intelligence: arguing the Orthogonality thesis --- Stuart_Armstrong --- 2012-05-15 --- https://www.lesswrong.com/posts/nvKZchuTW8zY6wvAj/general-purpose-intelligence-arguing-the-orthogonality,How many philosophers accept the orthogonality thesis ? Evidence from the PhilPapers survey --- Paperclip Minimizer --- 2018-06-16 --- https://www.lesswrong.com/posts/LdENjKB6G6fZDRZFw/how-many-philosophers-accept-the-orthogonality-thesis,Are we all misaligned? --- Mateusz Mazurkiewicz --- 2021-01-03 --- https://www.lesswrong.com/posts/Lshuoww97Loy2h7kw/are-we-all-misaligned-1,Orthogonality Thesis --- Yoav Ravid --- https://greaterwrong.com/tag/orthogonality-thesis?showPostCount=true&useTagName=true,"Arguing Orthogonality, published form --- Stuart_Armstrong --- 2013-03-18 --- https://www.lesswrong.com/posts/AJ3aP8iWxr6NaKi6j/arguing-orthogonality-published-form",John Danaher on ‘The Superintelligent Will’ --- lukeprog --- 2012-04-03 --- https://www.lesswrong.com/posts/DDxntjTYjRbZF9xTd/john-danaher-on-the-superintelligent-will
"What is ""instrumental convergence""?","Instrumental convergence refers to the idea that most sufficiently intelligent agents, including advanced AI systems, are likely to pursue certain instrumental goals regardless of their specific final goals [a, b]. These instrumental goals are considered convergent because their attainment increases the chances of achieving a wide range of final goals in various situations [b, d]. Some examples of instrumental goals include self-preservation and resource acquisition [a, b]. The concept of instrumental convergence suggests that even if we don't know an agent's exact final goal, we can still predict that it will pursue strategies aimed at obtaining more resources and increasing its capabilities [e]. It is important to note that while instrumental convergence is a widely discussed concept, there are also differing perspectives and critiques regarding its applicability to all intelligent agents [f].",Instrumental Convergence --- jimrandomh --- https://greaterwrong.com/tag/instrumental-convergence?showPostCount=true&useTagName=true,THE SUPERINTELLIGENT WILL: MOTIVATION AND INSTRUMENTAL RATIONALITY IN ADVANCED ARTIFICIAL AGENTS --- Nick Bostrom,"Review of ‘Debate on Instrumental Convergence between LeCun, Russell, Bengio, Zador, and More’ --- TurnTrout --- 2021-01-12 --- https://www.lesswrong.com/posts/ZPEGLoWMN242Dob6g/review-of-debate-on-instrumental-convergence-between-lecun","Superintelligence: Paths, Dangers, Strategies --- Nick Bostrom",Instrumental convergence --- arbital --- 2015-07-16 --- https://arbital.com/,Against Instrumental Convergence --- zulupineapple --- 2018-01-27 --- https://www.lesswrong.com/posts/28kcq8D4aCWeDKbBp/against-instrumental-convergence,"Book review: ""A Thousand Brains"" by Jeff Hawkins --- Steven Byrnes --- 2021-03-04 --- https://www.lesswrong.com/posts/ixZLTmFfnKRbaStA5/book-review-a-thousand-brains-by-jeff-hawkins",Superintelligence 10: Instrumentally convergent goals --- KatjaGrace --- 2014-11-18 --- https://www.lesswrong.com/posts/BD6G9wzRRt3fxckNC/superintelligence-10-instrumentally-convergent-goals
